üñºÔ∏è Image Caption Generator

This project is an Image Caption Generator built with Python using the BLIP (Bootstrapping Language-Image Pretraining) Conditional model. It automatically generates natural language descriptions for input images by leveraging state-of-the-art vision‚Äìlanguage modeling.

üöÄ Features

- Generate captions from input images

- Uses pretrained BLIP Conditional model

- Simple and modular Python implementation

- Reproducible pipeline for experimentation

- Academic report included

üß† Model

This project uses:

BLIP Conditional Generation Model

BLIP is a vision-language model designed for tasks like:

- Image captioning

- Visual question answering

- Vision-language understanding

It combines a Vision Transformer (ViT) encoder with a language model decoder.

‚öôÔ∏è Installation

Clone the repository:

git clone https://github.com/itu-itis24-demirozu23/image-caption-generator.git
cd image-caption-generator

Install dependencies:
pip install -r requirements.txt

‚ñ∂Ô∏è Usage

Run the script:
python caption_generator.py

Example workflow:

1)Provide an image path

2)Model processes the image

3)Generated caption is displayed

üõ†Ô∏è Technologies Used:

- Python

- PyTorch

- Hugging Face Transformers

- BLIP Model

- PIL / OpenCV (if used)

ü§ù Contributing
  
Contributions are welcome.
Feel free to open issues or submit pull requests.
